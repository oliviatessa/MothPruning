{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callin Switzer\n",
    "### 23 Aug 2018\n",
    "### Flatten Matlab structs\n",
    "### Use much more data than previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] \n",
      "\n",
      "last run on 2018-08-27 15:21:01.405355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import subprocess\n",
    "import csv\n",
    "\n",
    "print(sys.version, \"\\n\")\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"last run on \" + str(now))\n",
    "\n",
    "%qtconsole\n",
    "\n",
    "# define directories\n",
    "baseDir = os.getcwd()\n",
    "dataDir = r'D:\\MothSimulations\\11c-AggressiveManeuver\\Qstore\\hws_am_ntc'\n",
    "figDir = r'D:\\Dropbox\\mothMachineLearning_dataAndFigs\\Figs'\n",
    "dataOutput = r'D:\\Dropbox\\mothMachineLearning_dataAndFigs\\DataOutput'\n",
    "if not os.path.exists(dataOutput):\n",
    "    os.mkdir(dataOutput)\n",
    "    \n",
    "    \n",
    "# open file explorer\n",
    "# aa = r'explorer /select,\"'\n",
    "# subprocess.Popen(str(aa + figDir + '\\\"'))\n",
    "# subprocess.Popen(str(aa + dataDir + '\\\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qstore_1_hws_am_ntc.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qstore_2_hws_am_ntc.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qstore_3_hws_am_ntc.mat</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qstore_4_hws_am_ntc.mat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qstore_5_hws_am_ntc.mat</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0  1\n",
       "0  Qstore_1_hws_am_ntc.mat  1\n",
       "1  Qstore_2_hws_am_ntc.mat  2\n",
       "2  Qstore_3_hws_am_ntc.mat  3\n",
       "3  Qstore_4_hws_am_ntc.mat  4\n",
       "4  Qstore_5_hws_am_ntc.mat  5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list files\n",
    "fls = np.array(os.listdir(dataDir))\n",
    "simNum = [fls[ii].split(\"_\")[1] for ii in range(len(fls))]\n",
    "simNum =  [simNum[ii].zfill(2) for ii in range(len(simNum))]\n",
    "flDF = pd.DataFrame(np.vstack([fls, simNum]).transpose())\n",
    "flDF.sort_values(by = 1, inplace=True)\n",
    "flDF.reset_index(inplace = True, drop = True)\n",
    "flDF.iloc[:, 1] = flDF.iloc[:, 1].astype(int)\n",
    "print(len(flDF))\n",
    "flDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qstore_1_hws_am_ntc',\n",
       " 'Qstore_2_hws_am_ntc',\n",
       " 'Qstore_3_hws_am_ntc',\n",
       " 'Qstore_4_hws_am_ntc',\n",
       " 'Qstore_5_hws_am_ntc',\n",
       " 'Qstore_6_hws_am_ntc',\n",
       " 'Qstore_7_hws_am_ntc',\n",
       " 'Qstore_8_hws_am_ntc',\n",
       " 'Qstore_9_hws_am_ntc',\n",
       " 'Qstore_10_hws_am_ntc',\n",
       " 'Qstore_11_hws_am_ntc',\n",
       " 'Qstore_12_hws_am_ntc',\n",
       " 'Qstore_13_hws_am_ntc',\n",
       " 'Qstore_14_hws_am_ntc',\n",
       " 'Qstore_15_hws_am_ntc',\n",
       " 'Qstore_16_hws_am_ntc',\n",
       " 'Qstore_17_hws_am_ntc',\n",
       " 'Qstore_18_hws_am_ntc',\n",
       " 'Qstore_19_hws_am_ntc',\n",
       " 'Qstore_20_hws_am_ntc',\n",
       " 'Qstore_21_hws_am_ntc',\n",
       " 'Qstore_22_hws_am_ntc',\n",
       " 'Qstore_23_hws_am_ntc',\n",
       " 'Qstore_24_hws_am_ntc',\n",
       " 'Qstore_25_hws_am_ntc',\n",
       " 'Qstore_26_hws_am_ntc',\n",
       " 'Qstore_27_hws_am_ntc',\n",
       " 'Qstore_28_hws_am_ntc',\n",
       " 'Qstore_29_hws_am_ntc',\n",
       " 'Qstore_30_hws_am_ntc',\n",
       " 'Qstore_31_hws_am_ntc',\n",
       " 'Qstore_32_hws_am_ntc',\n",
       " 'Qstore_33_hws_am_ntc',\n",
       " 'Qstore_34_hws_am_ntc',\n",
       " 'Qstore_35_hws_am_ntc',\n",
       " 'Qstore_36_hws_am_ntc',\n",
       " 'Qstore_37_hws_am_ntc',\n",
       " 'Qstore_38_hws_am_ntc',\n",
       " 'Qstore_39_hws_am_ntc',\n",
       " 'Qstore_40_hws_am_ntc']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataToDo = np.array(flDF.loc[flDF.iloc[:,1] >= 1, 0])\n",
    "dataToDo = [dataToDo[jj].split(\".mat\")[0] for jj in range(len(dataToDo))]\n",
    "dataToDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "411.87004256248474\n",
      "436.33643531799316\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "418.7113356590271\n",
      "430.68215346336365\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "430.76696515083313\n",
      "442.52250123023987\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "431.8601472377777\n",
      "444.04470920562744\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "433.02167868614197\n",
      "444.9502441883087\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "409.63000679016113\n",
      "421.72765946388245\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "392.4241728782654\n",
      "404.42906427383423\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "391.8489730358124\n",
      "403.9655411243439\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "392.266806602478\n",
      "404.21883964538574\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "390.81172370910645\n",
      "402.5253927707672\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-8a90847e829e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msimNum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttRows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"alpha\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msimNum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttRows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tau0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msimNum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttRows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimNum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttRows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttRows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read in data and save\n",
    "for dataName in dataToDo:\n",
    "    Qmat = scipy.io.loadmat(os.path.join(dataDir, dataName+ \".mat\"))\n",
    "\n",
    "    # each timestep has 2500 Monte Carlo Simulations\n",
    "    # each file contains 100 timesteps\n",
    "\n",
    "    simNum = 2 # can go up to 2500\n",
    "    timestep = 99 # can go up to 99\n",
    "    tt = np.zeros((100, 13))\n",
    "    bigF = np.zeros((100*2500*100, 13))\n",
    "\n",
    "    stt = time.time()\n",
    "    ctr = 0\n",
    "\n",
    "    for timestep in range(100):\n",
    "        if np.mod(timestep, 10) == 0: \n",
    "            print(timestep)\n",
    "        vals = Qmat['Qstore'][timestep,0] #<-- set the array you want to access. \n",
    "        keys = Qmat['Qstore'][timestep,0].dtype.descr\n",
    "        keys = np.array(keys)[:, 0]\n",
    "\n",
    "        # Assemble the keys and values into variables with the same name as that used in MATLAB\n",
    "        for i in range(len(keys)):\n",
    "            key = keys[i]\n",
    "            val = np.squeeze(vals[key][0])  # squeeze is used to covert matlat (1,n) arrays into numpy (1,) arrays. \n",
    "            exec(key + '=val')\n",
    "\n",
    "        for simNum in range(2500):\n",
    "            #print(timestep, simNum)\n",
    "            ttRows = tt.shape[0]\n",
    "            tt[:, 0:8] = vals[\"bigQ\"][0, simNum]\n",
    "            tt[:, [8]] = np.repeat(np.concatenate(vals[\"F\"][0]).ravel()[simNum], ttRows).reshape(-1,1)\n",
    "            tt[:, [9]] = np.repeat(np.concatenate(vals[\"alpha\"][0]).ravel()[simNum], ttRows).reshape(-1,1)\n",
    "            tt[:, [10]] = np.repeat(np.concatenate(vals[\"tau0\"][0]).ravel()[simNum], ttRows).reshape(-1,1)\n",
    "            tt[:, [11]] =  np.repeat(simNum, ttRows).reshape(-1,1)\n",
    "            tt[:, [12]] = np.repeat(timestep, ttRows).reshape(-1,1)\n",
    "            bigF[ctr:ctr+ttRows, :] = tt\n",
    "            ctr += ttRows\n",
    "\n",
    "    print(time.time() - stt )\n",
    "\n",
    "    assert (not os.path.isfile(os.path.join(dataOutput, dataName + '.hdf5'))), \"File already exists\"\n",
    "    # save as hdf5 instead of csv (quite a bit faster)\n",
    "    bf2  = pd.DataFrame(bigF, columns=[\"x\", \"y\", \"theta\", \"phi\", \"x_dot\", \"y_dot\", \"theta_dot\", \"phi_dot\", \n",
    "                                       \"F\", \"alpha\", \"tau\", \"simNum\", \"timestep\"])\n",
    "\n",
    "    bf2.to_hdf(os.path.join(dataOutput, dataName + '.hdf5'), \"data\", mode='w')\n",
    "    print(time.time() - stt ) # 5 minutes to save a single hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "400/ 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt = time.time()\n",
    "bf2 = pd.read_csv(os.path.join(dataOutput, dataName + '.csv'))\n",
    "print(time.time() - stt ) # 52 seconds to read in\n",
    "bf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.unique(bf2.timestep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigF[0:100, :] = tt\n",
    "pd.DataFrame(bigF).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simNum = 2 # can go up to 2500\n",
    "timestep = 99 # can go up to 99\n",
    "vals = Qmat['Qstore'][timestep,0] #<-- set the array you want to access. \n",
    "keys = Qmat['Qstore'][timestep,0].dtype.descr\n",
    "keys = np.array(keys)[:, 0]\n",
    "\n",
    "# Assemble the keys and values into variables with the same name as that used in MATLAB\n",
    "for i in range(len(keys)):\n",
    "    key = keys[i]\n",
    "    val = np.squeeze(vals[key][0])  # squeeze is used to covert matlat (1,n) arrays into numpy (1,) arrays. \n",
    "    exec(key + '=val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = vals[\"bigQ\"][0, simNum]\n",
    "F =  np.repeat(np.concatenate(vals[\"F\"][0]).ravel()[simNum], 100).reshape(-1,1)\n",
    "alpha = np.repeat(np.concatenate(vals[\"alpha\"][0]).ravel()[simNum], 100).reshape(-1,1)\n",
    "tau0 = np.repeat(np.concatenate(vals[\"tau0\"][0]).ravel()[simNum], 100).reshape(-1,1)\n",
    "simNum_ =  np.repeat(simNum, 100).reshape(-1,1)\n",
    "timestep_ = np.repeat(timestep, 100).reshape(-1,1)\n",
    "pd.DataFrame(np.hstack([tt, F, alpha, tau0, simNum_, timestep_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dataOutput, dataName + '.csv'), 'a') as f:\n",
    "    np.savetxt(f, np.hstack([tt, F, alpha, tau0, simNum_, timestep_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(dataOutput, dataName + \".csv\"), np.hstack([tt, F, alpha, tau0, simNum_, timestep_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack([tt, F, alpha, tau0, simNum_, timestep_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simNum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd['timestep'] = (np.floor(np.arange(0, 100, step = 0.01))).astype(int)\n",
    "cols = dd.columns.tolist()\n",
    "dd = dd[[cols[-1]] + cols[:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = np.zeros(Qdta.shape).reshape((-1, 100*8))\n",
    "cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another possible way to rearrange the data\n",
    "stt = time.time()\n",
    "\n",
    "ss = 8 # number of columns to repeat\n",
    "cctr = 0 # output row counter\n",
    "\n",
    "\n",
    "rctr = 0 # row counter for input data\n",
    "    \n",
    "while rctr < Qdta.shape[0]:\n",
    "    \n",
    "    ii = 0 # column counter for input data\n",
    "    while ii < Qdta.shape[1]:\n",
    "        a = np.array(dd.iloc[rctr:(rctr + 100), ii + 2:ii + 2 + ss])\n",
    "        bb = np.array(np.zeros(a.shape)).reshape([1, -1])\n",
    "        bb.shape\n",
    "\n",
    "        for kk in np.arange(0, a.shape[0], step = 1):\n",
    "            bb[0, kk*ss:(kk+1)*ss] = a[kk, 0:ss]  \n",
    "\n",
    "        ii += ss\n",
    "    \n",
    "        cc[cctr, :] = bb\n",
    "        cctr += 1\n",
    "    # update row counter\n",
    "    print(str(cctr) + \" of \" + str(cc.shape[0]))\n",
    "    rctr += 100\n",
    "\n",
    "    \n",
    "print(str(np.round(time.time() - stt)) + \" Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,100))\n",
    "plt.imshow(cc, cmap='hot', interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "for ii in np.arange(5000, 10000):\n",
    "    plt.plot(cc[ii, np.arange(0, 800, step = 8)], cc[ii, np.arange(1, 800, step = 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make data frame and name columns\n",
    "tsDf = pd.DataFrame(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsDf[\"timestep\"] = np.floor(np.arange(0,tsDf.shape[0]) / 2500).astype(int)\n",
    "tsDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_cols = [\"x\", \"y\", \"theta\", \"phi\", \"x_dot\", \"y_dot\", \"theta_dot\", \"phi_dot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jj = 0\n",
    "hh = []\n",
    "for jj in range(100):\n",
    "    hh.append([new_cols[ii] +\"_\" +str(jj) for ii in range(8)])\n",
    "\n",
    "allCols = np.concatenate(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsDf.columns = np.append(allCols, \"timestep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forceAngle = np.transpose(Vmat[\"ValSp3\"])\n",
    "fa = forceAngle.reshape((250000, 12), order = \"C\" )\n",
    "fa2=  pd.DataFrame(fa)\n",
    "fa2.columns = np.append([new_cols[ii] +\"_\" +str(99) for ii in range(8)], [\"F\", \"alpha\", \"tau\", \"cost\"])\n",
    "fa2[\"Fx\"] = fa2.F * np.cos(fa2.alpha)\n",
    "fa2[\"Fy\"] = fa2.F * np.sin(fa2.alpha)\n",
    "fa2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combDF = pd.merge(tsDf.dropna(), fa2.dropna(), \n",
    "                  left_on = [new_cols[ii] +\"_\" +str(99) for ii in range(8)], \n",
    "                  right_on = [new_cols[ii] +\"_\" +str(99) for ii in range(8)], \n",
    "                  how = \"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how many NA's were dropped\n",
    "fa2.shape[0] - fa2.dropna().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished data munging\n",
    "\n",
    "Now fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = combDF.iloc[:, np.concatenate((np.arange(0,8), np.arange(792,800)))].copy()\n",
    "Y = combDF.iloc[:, [803, 805, 806]]\n",
    "\n",
    "# # subtract x_0 and y_0 from everything\n",
    "X.x_99 = X.x_99 - X.x_0\n",
    "X.y_99 = X.y_99 - X.y_0\n",
    "\n",
    "X.x_0 = X.y_0 = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refref: scale test and training separately:\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scalerX = StandardScaler()  \n",
    "scalerY = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scalerX.fit(Xtrain)  \n",
    "scalerY.fit(Ytrain) \n",
    "\n",
    "Xtrain_scaled = scalerX.transform(Xtrain)  \n",
    "Ytrain_scaled = scalerY.transform(Ytrain)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "Xtest_scaled = scalerX.transform(Xtest)\n",
    "Ytest_scaled = scalerY.transform(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcheck = scalerX.inverse_transform(Xtrain_scaled)\n",
    "np.allclose(xcheck, np.array(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save scalers\n",
    "import pickle\n",
    "\n",
    "# with open(os.path.join(dataDir, \"StandardScaler_zerod.pkl\"), 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([scalerX, scalerY], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del scalerX\n",
    "# del scalerY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Getting back the objects:\n",
    "# with open(os.path.join(dataDir, \"StandardScaler_zerod.pkl\"), 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "#     scalerX, scalerY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scaler\n",
    "plt.hist(Xtrain.iloc[:, 3])\n",
    "plt.show()\n",
    "plt.hist(Xtrain_scaled[:,3])\n",
    "plt.show()\n",
    "\n",
    "xcheck = scalerX.inverse_transform(Xtrain_scaled)\n",
    "plt.hist(xcheck[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape,  Ytrain.shape, Xtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stt = time.time()\n",
    "# mor = MultiOutputRegressor(RandomForestRegressor(500, n_jobs = 4)).fit(Xtrain_scaled, Ytrain_scaled)\n",
    "# print(time.time() - stt) # ~ 13 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # save RF model\n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(mor, os.path.join(dataDir, \"FitRF_Scaled2.pkl\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mpreds = mor.predict(Xtest_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.close(\"all\")\n",
    "# fig, axs = plt.subplots(1,3, figsize=(15, 5), facecolor='w', edgecolor='k')\n",
    "# fig.subplots_adjust(hspace = 0.1, wspace=0.3)\n",
    "# fig.suptitle('Predicted vs. acutal for Mulit-output Random Forest Regressor (500 trees)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# axs = axs.ravel()\n",
    "# nms = [\"tau\", \"Fx\", \"Fy\"]\n",
    "\n",
    "\n",
    "# for ii in range(3):\n",
    "#     # read in image\n",
    "    \n",
    "#     try:\n",
    "#         axs[ii].scatter(y = Ytest_scaled[:,ii],x = mpreds[:,ii], s = 1, c= 'indianred' )\n",
    "#         axs[ii].set_xlabel(\"Predicted Value (Scaled)\")\n",
    "#         if(ii == 0):\n",
    "#             axs[ii].set_ylabel(\"Actual Value\\n(Scaled)\")\n",
    "#             axs[ii].set_xlim([-10, 5])\n",
    "#         axs[ii].set_title(nms[ii])\n",
    "#         axs[ii].plot(Ytest_scaled[:,ii], Ytest_scaled[:,ii], 'maroon')\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "    \n",
    "# fig.savefig(os.path.join(figDir, \"RFpreds_Scaled2.png\"), dpi = 120, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-output neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stt = time.time()\n",
    "# nn1 = MLPRegressor(solver='lbfgs', alpha=0,\n",
    "#                   hidden_layer_sizes=(100, 6), max_iter=10000, \n",
    "#                   activation='tanh', verbose = True)\n",
    "# nnMod = MultiOutputRegressor(nn1).fit(Xtrain_scaled, Ytrain_scaled)\n",
    "# print(time.time() - stt) # takes 128 minutes on MBPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save nn, so I can reload without retraining\n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(nnMod, os.path.join(dataDir, \"TrainedNN_scaled_zerod.pkl\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load trained nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "nnMod = joblib.load(os.path.join(dataDir, \"TrainedNN_scaled_zerod.pkl\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnpreds = nnMod.predict(Xtest_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(1,3, figsize=(15, 5), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.3)\n",
    "fig.suptitle('Predicted vs. acutal for 2-hidden-layer Neural Network', fontsize=14, fontweight='bold')\n",
    "\n",
    "axs = axs.ravel()\n",
    "nms = [\"tau\", \"Fx\", \"Fy\"]\n",
    "\n",
    "\n",
    "for ii in range(3):\n",
    "    # read in image\n",
    "    \n",
    "    try:\n",
    "        axs[ii].scatter(y = Ytest_scaled[:,ii],x = nnpreds[:,ii], s = 1 )\n",
    "        axs[ii].set_xlabel(\"Predicted Value (scaled)\")\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\\n(scaled)\")\n",
    "        axs[ii].set_title(nms[ii])\n",
    "        axs[ii].plot(Ytest_scaled[:,ii], Ytest_scaled[:,ii], 'b')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "fig.savefig(os.path.join(figDir, \"NNPreds2_zerod.png\"), dpi = 120, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare simulation vs. nn preds for test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.concat([Xtest, Ytest], axis = 1).reset_index(drop = True)\n",
    "\n",
    "shuff2 = s1\n",
    "\n",
    "# predictions\n",
    "XX_scaled = scalerX.transform(shuff2.iloc[:, 0:16])\n",
    "nnpreds3 = nnMod.predict(XX_scaled)\n",
    "\n",
    "# add preds back to data frame\n",
    "nn_back = scalerY.inverse_transform(nnpreds3)\n",
    "preds1 = pd.DataFrame(nn_back)\n",
    "preds1.columns = [\"tau_pred\", \"Fx_pred\", \"Fy_pred\"]\n",
    "preds1[\"F_pred\"] = np.sqrt(preds1.Fx_pred **2 + preds1.Fy_pred **2)\n",
    "\n",
    "\n",
    "pre_alpha = np.arctan2(preds1.Fy_pred, preds1.Fx_pred )\n",
    "alpha = pre_alpha.copy()\n",
    "\n",
    "for ii in pre_alpha.index:\n",
    "    if (preds1.Fy_pred[ii] > 0) :\n",
    "        alpha[ii] = pre_alpha[ii]\n",
    "    else:\n",
    "        alpha[ii] = (2*np.pi) + pre_alpha[ii]\n",
    "        \n",
    "preds1[\"alpha_pred\"] = alpha\n",
    "\n",
    "\n",
    "preds1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_1 = pd.concat([shuff2, preds1], axis = 1)\n",
    "s2_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_1.to_csv(os.path.join(dataDir, \"simulationData5_withFinal_testset.csv\"), index=False)\n",
    "print(dataDir)\n",
    "\n",
    "simSet1 = s2_1.loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0', 'tau_pred','F_pred','alpha_pred']]\n",
    "\n",
    "\n",
    "simSet1.to_csv(os.path.join(dataDir, \"simulationDataset5_blinded_testset.csv\"), index=False)\n",
    "simSet1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simSet1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a new dataset, that is similar to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = Xtrain.std(axis = 0)\n",
    "means = Xtrain.mean(axis = 0)\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# Note, arguments are norm.pdf(xvals,mean, std dev))\n",
    "\n",
    "sampList = []\n",
    "\n",
    "ii = 0\n",
    "\n",
    "for ii in range(len(stds.index)):\n",
    "    indd = stds.index[ii]\n",
    "    samps = np.random.normal(loc=means[indd], scale=stds[indd], size=1000)\n",
    "    sampList.append(samps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatedDatset = pd.DataFrame(np.transpose(sampList), columns = stds.index).reset_index(drop = True)\n",
    "generatedDatset.head()\n",
    "generatedDatset.loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(Xtrain.sample(5000, random_state = 123).loc[:, ['theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0']].reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(Xtrain.sample(5000, random_state = 123).loc[:, ['x_99', 'y_99','theta_99', 'phi_99', 'x_dot_99', 'y_dot_0', 'theta_dot_99',\n",
    "       'phi_dot_99']].reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatedDatset = pd.DataFrame(np.transpose(sampList), columns = stds.index).reset_index(drop = True)\n",
    "generatedDatset.loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0']] = Xtrain.sample(1000, random_state = 123).loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0']].reset_index(drop = True)\n",
    "\n",
    "generatedDatset = Xtrain.sample(1000, random_state = 123).reset_index(drop = True)\n",
    "generatedDatset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for generated data\n",
    "# predictions\n",
    "XX_scaled = scalerX.transform(generatedDatset)\n",
    "nnpreds3 = nnMod.predict(XX_scaled)\n",
    "\n",
    "# add preds back to data frame\n",
    "nn_back = scalerY.inverse_transform(nnpreds3)\n",
    "preds1 = pd.DataFrame(nn_back)\n",
    "preds1.columns = [\"tau_pred\", \"Fx_pred\", \"Fy_pred\"]\n",
    "preds1[\"F_pred\"] = np.sqrt(preds1.Fx_pred **2 + preds1.Fy_pred **2)\n",
    "\n",
    "\n",
    "pre_alpha = np.arctan2(preds1.Fy_pred, preds1.Fx_pred )\n",
    "alpha = pre_alpha.copy()\n",
    "\n",
    "for ii in pre_alpha.index:\n",
    "    if (preds1.Fy_pred[ii] > 0) :\n",
    "        alpha[ii] = pre_alpha[ii]\n",
    "    else:\n",
    "        alpha[ii] = (2*np.pi) + pre_alpha[ii]\n",
    "        \n",
    "preds1[\"alpha_pred\"] = alpha\n",
    "\n",
    "\n",
    "preds1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5 = pd.concat([generatedDatset, preds1], axis = 1)\n",
    "s5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Ytrain.tau)\n",
    "plt.show()\n",
    "plt.hist(s5.tau_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s5.to_csv(os.path.join(dataDir, \"simulationData6_withFinal_generatedtestset.csv\"), index=False)\n",
    "print(dataDir)\n",
    "\n",
    "simSet1 = s5.loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0', 'tau_pred','F_pred','alpha_pred']]\n",
    "\n",
    "\n",
    "simSet1.to_csv(os.path.join(dataDir, \"simulationDataset6_blinded_generatedtestset.csv\"), index=False)\n",
    "simSet1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw samples from normal dist'n\n",
    "samps = np.random.normal(loc=means['theta_0'], scale=stds['theta_0'], size=10000)\n",
    "sns.kdeplot(samps, bw=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(Xtrain.theta_0, bw=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refref: run simulation vs. nn preds for test set!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create dataset with same initial conditions and shuffle output conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = combDF.iloc[:, np.hstack([np.arange(0,8),np.arange(792,800), [801, 802, 803]])].sample(1000, random_state = 1234)\n",
    "s1.reset_index(inplace = True)\n",
    "\n",
    "# set x_0 and y_0 to (0,0)  -- refref: may want to do this for training, actually\n",
    "s1.x_99 = s1.x_99 - s1.x_0\n",
    "s1.y_99 = s1.y_99 - s1.y_0\n",
    "\n",
    "s1.x_0 = 0\n",
    "s1.y_0 = 0\n",
    "\n",
    "s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sTemp = s1.loc[:, ['x_99', 'y_99', 'theta_99', 'phi_99',\n",
    "       'x_dot_99', 'y_dot_99', 'theta_dot_99', 'phi_dot_99']]\n",
    "sInit = s1.loc[:, ['index', 'x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0',\n",
    "       'theta_dot_0', 'phi_dot_0']]\n",
    "\n",
    "shuff = sTemp.sample(sTemp.shape[0], random_state = 12346)\n",
    "shuff2 = pd.concat([sInit, shuff.reset_index(drop = True)], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "shuff2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "XX_scaled = scalerX.transform(shuff2.iloc[:, 1:17])\n",
    "nnpreds3 = nnMod.predict(XX_scaled)\n",
    "\n",
    "# add preds back to data frame\n",
    "nn_back = scalerY.inverse_transform(nnpreds3)\n",
    "preds1 = pd.DataFrame(nn_back)\n",
    "preds1.columns = [\"tau_pred\", \"Fx_pred\", \"Fy_pred\"]\n",
    "preds1[\"F_pred\"] = np.sqrt(preds1.Fx_pred **2 + preds1.Fy_pred **2)\n",
    "\n",
    "\n",
    "pre_alpha = np.arctan2(preds1.Fy_pred, preds1.Fx_pred )\n",
    "alpha = pre_alpha.copy()\n",
    "\n",
    "for ii in pre_alpha.index:\n",
    "    if (preds1.Fy_pred[ii] > 0) :\n",
    "        alpha[ii] = pre_alpha[ii]\n",
    "    else:\n",
    "        alpha[ii] = (2*np.pi) + pre_alpha[ii]\n",
    "        \n",
    "preds1[\"alpha_pred\"] = alpha\n",
    "\n",
    "\n",
    "preds1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_1 = pd.concat([shuff2, preds1], axis = 1)\n",
    "s2_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_1.to_csv(os.path.join(dataDir, \"simulationData4_withFinal.csv\"), index=False)\n",
    "print(dataDir)\n",
    "\n",
    "simSet1 = s2_1.loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0', 'tau_pred','F_pred','alpha_pred']]\n",
    "\n",
    "\n",
    "simSet1.to_csv(os.path.join(dataDir, \"simulationDataset4_blinded.csv\"), index=False)\n",
    "simSet1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refref: might get better predictions if we set initial X and Y to (0,0) and\n",
    "# and transform output X and Y relative to the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Dataset to check in Jorge's simulation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## generate dataset for Jorge to send to model\n",
    "s1 = combDF.iloc[:, np.hstack([np.arange(0,8),np.arange(792,800), [801, 802, 803]])].sample(1000)\n",
    "s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "XX_scaled = scalerX.transform(s1.iloc[:, 0:16])\n",
    "nnpreds2 = nnMod.predict(XX_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1.iloc[:, 0:16].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1[\"Fx\"] = s1.F * np.cos(s1.alpha)\n",
    "s1[\"Fy\"] = s1.F * np.sin(s1.alpha)\n",
    "s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute F, alpha, to double check\n",
    "s1[\"F_\"] = np.sqrt(s1.Fx **2 + s1.Fy **2)\n",
    "\n",
    "\n",
    "pre_alpha = np.arctan2(s1.Fy, s1.Fx )\n",
    "alpha = pre_alpha.copy()\n",
    "\n",
    "for ii in pre_alpha.index:\n",
    "    if (s1.Fy[ii] > 0) :\n",
    "        alpha[ii] = pre_alpha[ii]\n",
    "    else:\n",
    "        alpha[ii] = (2*np.pi) + pre_alpha[ii]\n",
    "        \n",
    "s1[\"alpha_\"] = alpha\n",
    "s1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "YY_scaled = scalerY.transform(s1.iloc[:, [18, 19, 20]])\n",
    "pd.DataFrame(YY_scaled).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot to double-check\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(1,3, figsize=(15, 5), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.3)\n",
    "fig.suptitle('Predicted vs. acutal for subset', fontsize=14, fontweight='bold')\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "# convert back to original values\n",
    "nn_back = scalerY.inverse_transform(nnpreds2)\n",
    "YY_orig = s1.iloc[:, [18, 19, 20]]\n",
    "\n",
    "\n",
    "for ii in range(3):\n",
    "    # read in image\n",
    "    \n",
    "    try:\n",
    "        axs[ii].scatter(y = YY_orig.iloc[:,ii],x = nn_back[:,ii], s = 1 )\n",
    "        axs[ii].set_xlabel(\"Predicted Value (scaled)\")\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\\n(scaled)\")\n",
    "        axs[ii].set_title(nms[ii])\n",
    "        axs[ii].plot(nn_back[:,ii], nn_back[:,ii], 'b')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add predictions to data frame\n",
    "preds1 = pd.DataFrame(nn_back)\n",
    "preds1.columns = [\"tau_pred\", \"Fx_pred\", \"Fy_pred\"]\n",
    "preds1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc2 = pd.concat([s1.reset_index(drop=True), preds1.reset_index(drop=True)], axis = 1)\n",
    "cc2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc2[\"F_pred\"] = np.sqrt(cc2.Fx_pred **2 + cc2.Fy_pred **2)\n",
    "\n",
    "\n",
    "pre_alpha = np.arctan2(cc2.Fy_pred, cc2.Fx_pred )\n",
    "alpha = pre_alpha.copy()\n",
    "\n",
    "for ii in pre_alpha.index:\n",
    "    if (cc2.Fy_pred[ii] > 0) :\n",
    "        alpha[ii] = pre_alpha[ii]\n",
    "    else:\n",
    "        alpha[ii] = (2*np.pi) + pre_alpha[ii]\n",
    "        \n",
    "cc2[\"alpha_pred\"] = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc2.to_csv(os.path.join(dataDir, \"simulationData1_withFinal.csv\"), index=False)\n",
    "print(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simSet1 = cc2.loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0', 'tau_pred','F_pred','alpha_pred']]\n",
    "simSet1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simSet1.to_csv(os.path.join(dataDir, \"simulationDataset1_blinded.csv\"), index=False)\n",
    "print(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Dataset #2 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## now build another training dataset, that is random combinations of initial values\n",
    "\n",
    "def shuffle(df, n=1, axis=0):     \n",
    "    df = df.copy()\n",
    "    for _ in range(n):\n",
    "        df.apply(np.random.shuffle, axis=axis)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = combDF.iloc[:, np.hstack([np.arange(0,8),np.arange(792,800)])].sample(1500)\n",
    "ss.reset_index(drop = True, inplace = True)\n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2 = shuffle(ss)\n",
    "s2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict on shuffled data\n",
    "s2_scaled = scalerX.transform(s2)\n",
    "nnpreds3 = nnMod.predict(s2_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add preds back to data frame\n",
    "nn_back = scalerY.inverse_transform(nnpreds3)\n",
    "preds1 = pd.DataFrame(nn_back)\n",
    "preds1.columns = [\"tau_pred\", \"Fx_pred\", \"Fy_pred\"]\n",
    "preds1[\"F_pred\"] = np.sqrt(preds1.Fx_pred **2 + preds1.Fy_pred **2)\n",
    "\n",
    "\n",
    "pre_alpha = np.arctan2(preds1.Fy_pred, preds1.Fx_pred )\n",
    "alpha = pre_alpha.copy()\n",
    "\n",
    "for ii in pre_alpha.index:\n",
    "    if (preds1.Fy_pred[ii] > 0) :\n",
    "        alpha[ii] = pre_alpha[ii]\n",
    "    else:\n",
    "        alpha[ii] = (2*np.pi) + pre_alpha[ii]\n",
    "        \n",
    "preds1[\"alpha_pred\"] = alpha\n",
    "\n",
    "\n",
    "preds1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2_1 = pd.concat([s2, preds1], axis = 1)\n",
    "s2_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2_1.to_csv(os.path.join(dataDir, \"simulationData2_withFinal.csv\"), index=False)\n",
    "\n",
    "simSet2 = s2_1.loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0', 'tau_pred','F_pred','alpha_pred']]\n",
    "\n",
    "simSet2.to_csv(os.path.join(dataDir, \"simulationDataset2_blinded.csv\"), index=False)\n",
    "simSet2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ Dataset 3 -- Sample initial and get randomized final states \n",
    "\n",
    "#\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = combDF.iloc[:, np.hstack([np.arange(0,8),np.arange(792,800)])].sample(1200)\n",
    "ss.reset_index(drop = True, inplace = True)\n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only mix up possible output states\n",
    "s3_1 = shuffle(ss.iloc[:, 8:])\n",
    "s3_2 = pd.concat([ss.iloc[:,0:8], s3_1], axis = 1)\n",
    "s3_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict on shuffled data\n",
    "s3_scaled = scalerX.transform(s3_2)\n",
    "nnpreds3_1 = nnMod.predict(s3_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add preds back to data frame\n",
    "nn_back = scalerY.inverse_transform(nnpreds3_1)\n",
    "preds1 = pd.DataFrame(nn_back)\n",
    "preds1.columns = [\"tau_pred\", \"Fx_pred\", \"Fy_pred\"]\n",
    "preds1[\"F_pred\"] = np.sqrt(preds1.Fx_pred **2 + preds1.Fy_pred **2)\n",
    "\n",
    "\n",
    "pre_alpha = np.arctan2(preds1.Fy_pred, preds1.Fx_pred )\n",
    "alpha = pre_alpha.copy()\n",
    "\n",
    "for ii in pre_alpha.index:\n",
    "    if (preds1.Fy_pred[ii] > 0) :\n",
    "        alpha[ii] = pre_alpha[ii]\n",
    "    else:\n",
    "        alpha[ii] = (2*np.pi) + pre_alpha[ii]\n",
    "        \n",
    "preds1[\"alpha_pred\"] = alpha\n",
    "\n",
    "\n",
    "preds1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3_4 = pd.concat([s3_2, preds1], axis = 1)\n",
    "s3_4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3_4.to_csv(os.path.join(dataDir, \"simulationData3_withFinal.csv\"), index=False)\n",
    "\n",
    "simSet3 = s3_4.loc[:, ['x_0', 'y_0', 'theta_0', 'phi_0', 'x_dot_0', 'y_dot_0', 'theta_dot_0',\n",
    "       'phi_dot_0', 'tau_pred','F_pred','alpha_pred']]\n",
    "\n",
    "simSet3.to_csv(os.path.join(dataDir, \"simulationDataset3_blinded.csv\"), index=False)\n",
    "simSet3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## nn practice\n",
    "\n",
    "Xx = np.transpose(np.array([np.arange(-10, 10, step = 0.001), np.random.uniform(size = 20000)]))\n",
    "Xx_scaled = sklearn.preprocessing.scale(Xx, axis = 0)\n",
    "Yy = np.sin(Xx[:,0]) *4 **2 + 50\n",
    "Yy_scaled = sklearn.preprocessing.scale(Yy, axis = 0)\n",
    "plt.plot(Xx_scaled[:,0], Yy)\n",
    "plt.show()\n",
    "plt.scatter(Xx_scaled[:,1], Yy, s = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(Xx_scaled[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stt = time.time()\n",
    "nn1 = MLPRegressor(solver='lbfgs', alpha=0,\n",
    "                  hidden_layer_sizes=(10,10), max_iter=1000, \n",
    "                  verbose = True, tol=0.0000100,\n",
    "                  activation='tanh')\n",
    "nnMod = nn1.fit(Xx_scaled, Yy)\n",
    "print(time.time() - stt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnpreds = nnMod.predict(Xx_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x = Xx_scaled[:,0], y = Yy)\n",
    "plt.scatter(x = Xx_scaled[:,0], y = nnpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x = Xx_scaled[:,0], y = Yy_scaled)\n",
    "plt.scatter(x = Xx_scaled[:,0], y = nnpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x = nnpreds, y = Yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge with F, alpha, tau\n",
    "Vdta = Vmat[\"ValSp3\"]\n",
    "dd2 = pd.DataFrame(Vdta)\n",
    "dd2.columns = np.append([new_cols[ii] +\"_\" +str(99) for ii in range(8)], [\"F\", \"alpha\", \"tau\"])\n",
    "print(dd2.shape)\n",
    "dd2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ff = dd\n",
    "\n",
    "\n",
    "stp = 8\n",
    "for stp in np.arange(10):\n",
    "    xx = 1\n",
    "    yy = 2\n",
    "    for ii in np.arange(1000):\n",
    "        plt.scatter(ff.iloc[stp*100:(stp+1)*100,xx], ff.iloc[stp*100:(stp+1)*100, yy], s = 1)\n",
    "        xx += 8\n",
    "        yy +=8\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ii = 200\n",
    "\n",
    "for jj in np.arange(0, dta.shape[0], step = 8):\n",
    "    plt.scatter(dta[ii:ii+100, jj], dta[ii:ii+100, jj + 1], s = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_cols = [\"x\", \"y\", \"theta\", \"phi\", \"x_dot\", \"y_dot\", \"theta_dot\", \"phi_dot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# another possible way to rearrange the data\n",
    "ss = 8\n",
    "a = dta\n",
    "bb = np.array(np.zeros(a.shape)).reshape([-1, ss])\n",
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.shape[1]\n",
    "a.shape[0]\n",
    "pd.DataFrame(a[0:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for kk in np.arange(0, a.shape[1], step = a.shape[0]):\n",
    "    bb[kk:(kk+a.shape[0]),0:ss] = a[0:, kk:(ss+kk)]  \n",
    "\n",
    "    \n",
    "cc = pd.DataFrame(bb)\n",
    "cc.columns = new_cols\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ii in np.arange(0, 10000, step = 100):\n",
    "    plt.scatter(cc.loc[ii:ii+100, \"x\"],cc.loc[ii:ii+100, \"y\"], s = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd2 = dta.reshape((-1, 8))\n",
    "dd3 = pd.DataFrame(dd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd3.columns =  [\"x\", \"y\", \"theta\", \"phi\", \"x_dot\", \"y_dot\", \"theta_dot\", \"phi_dot\"]\n",
    "dd3.head()\n",
    "\n",
    "plt.scatter(dd3.loc[1:10000, \"x\"],dd3.loc[1:10000, \"y\"], s = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dta.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_cols = [\"x\", \"y\", \"theta\", \"phi\", \"x_dot\", \"y_dot\", \"theta_dot\", \"phi_dot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initialCond = dta[np.arange(0, dta.shape[0], step = 100),: ]\n",
    "finalCond = dta[np.arange(99, dta.shape[0], step = 100), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape data\n",
    "i2 = initialCond.reshape((250000, 8))\n",
    "f2 = finalCond.reshape((250000, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(i2[2495:2555, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conddta = np.hstack((i2, f2))\n",
    "condDta2 = pd.DataFrame(conddta)\n",
    "condDta2.columns = np.concatenate(([ii + \"_i\" for ii in new_cols], [ii + \"\" for ii in new_cols]))\n",
    "print(condDta2.shape)\n",
    "condDta2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vmat = scipy.io.loadmat(os.path.join(dataDir, \"ValSp3_1_am_con.mat\"))\n",
    "Vmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this might work\n",
    "forceAngle = np.transpose(Vmat[\"ValSp3\"])\n",
    "fa = forceAngle.reshape((250000, 12), order = \"C\" )\n",
    "fa2=  pd.DataFrame(fa)\n",
    "fa2.columns = np.concatenate((new_cols, [\"F\", \"alpha\", \"tau\", \"cost\"]))\n",
    "fa2[\"Fx\"] = fa2.F * np.cos(fa2.alpha)\n",
    "fa2[\"Fy\"] = fa2.F * np.sin(fa2.alpha)\n",
    "fa2\n",
    "combDF = pd.merge(condDta2.dropna(), fa2.dropna(), left_on = new_cols, right_on = new_cols, how = \"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combDF.dropna().iloc[:, 0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop NA's\n",
    "X = np.array(combDF.dropna().iloc[:, 0:16])\n",
    "Y = np.array(combDF.dropna().iloc[:, [18, 20, 21]])\n",
    "\n",
    "# check: refref: not checked\n",
    "print(all(X[:,9] == Y[:,1]))\n",
    "print(len(X))\n",
    "\n",
    "#Y = Y[:, [10, 12, 13]]\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "mor = MultiOutputRegressor(RandomForestRegressor(1000, n_jobs = 4)).fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpreds = mor.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nms = [\"tau\", \"Fx\", \"Fy\"]\n",
    "\n",
    "for ii in range(3):\n",
    "    plt.scatter(y = Ytest[:,ii],x = mpreds[:,ii], s = 2 )\n",
    "    plt.xlabel(\"Predicted Value\")\n",
    "    plt.ylabel(\"Actual Value\")\n",
    "    plt.title(nms[ii])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.vstack((np.arange(8), np.arange(7, 15), np.arange(7, 15)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = 2\n",
    "bb = np.array(np.zeros(a.shape)).reshape([-1, ss])\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.arange(0, a.shape[1], step = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = 2\n",
    "\n",
    "for kk in np.arange(0, a.shape[1], step = a.shape[0]):\n",
    "    bb[kk:(kk+a.shape[0]),0:ss] = a[0:, kk:(ss+kk)]  \n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb[kk:(kk+ss),0:ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss =2\n",
    "kk = 2\n",
    "\n",
    "a[0:, kk:(ss+kk)]\n",
    "bb[kk:(kk+a.shape[0]),0:ss]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refref: figure out how to reshape this\n",
    "\n",
    "a.reshape((-1, 2), order = \"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.reshape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(fa)\n",
    "print(dd.shape)\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = dd.transpose()\n",
    "fa.columns = np.concatenate((new_cols, [\"F\", \"alpha\", \"tau\", \"cost\"]))\n",
    "print(fa.shape)\n",
    "fa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make Fx and Fy\n",
    "fa[\"Fx\"] = fa.F * np.cos(fa.alpha)\n",
    "fa[\"Fy\"] = fa.F * np.sin(fa.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ii in np.arange(1, spray.shape[1], step = 8):\n",
    "\n",
    "    tmparr = np.hstack((sparray[0, ii:ii+8], sparray[99, ii:ii+8]))\n",
    "\n",
    "    if ii == 1:\n",
    "        newArr = tmparr\n",
    "    else: \n",
    "        newArr = np.vstack((newArr, tmparr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF = pd.DataFrame(newArr)\n",
    "newDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDF.columns = np.concatenate(([ii + \"_i\" for ii in new_cols], [ii + \"\" for ii in new_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(newDF.shape)\n",
    "newDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop NA's\n",
    "X = np.array(newDF.dropna())\n",
    "Y = np.array(fa.dropna())\n",
    "\n",
    "# check\n",
    "print(all(X[:,9] == Y[:,1]))\n",
    "print(len(X))\n",
    "\n",
    "Y = Y[:, [10, 12, 13]]\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y[0:10, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combDF = pd.merge(newDF.dropna(), fa.dropna(), left_on = new_cols, right_on = new_cols, how = \"inner\")\n",
    "print(combDF.shape)\n",
    "combDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into test and training set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf1 = RandomForestRegressor(1000, n_jobs = 4).fit(Xtrain, Ytrain)\n",
    "Ypred1 = clf1.predict(Xtest)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Ypred1[:, 0], Ypred1[:, 1], 'o', alpha=0.5)\n",
    "ax.set_xlabel('$y_1$'); ax.set_ylabel('$y_2$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ii in range(3):\n",
    "    plt.scatter(Ytest[:,ii],Ypred1[:,ii] )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiOutputRF(object):\n",
    "    \n",
    "    '''From here: http://astrohackweek.org/blog/multi-output-random-forests.html'''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        X, Y = map(np.atleast_2d, (X, Y))\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        Ny = Y.shape[1]\n",
    "        \n",
    "        self.clfs = []\n",
    "        for i in range(Ny):\n",
    "            clf = RandomForestRegressor(*self.args, **self.kwargs)\n",
    "            Xi = np.hstack([X, Y[:, :i]])\n",
    "            yi = Y[:, i]\n",
    "            self.clfs.append(clf.fit(Xi, yi))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Y = np.empty([X.shape[0], len(self.clfs)])\n",
    "        for i, clf in enumerate(self.clfs):\n",
    "            Y[:, i] = clf.predict(np.hstack([X, Y[:, :i]]))\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf2 = MultiOutputRF(1000, n_jobs = 4).fit(Xtrain, Ytrain)\n",
    "Ypred2 = clf2.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: might want to break down into Fx and Fy, b/c radians don't work well with MSE\n",
    "\n",
    "nms = [\"tau\", \"Fx\", \"Fy\"]\n",
    "\n",
    "for ii in range(3):\n",
    "    plt.scatter(y = Ytest[:,ii],x = Ypred2[:,ii], s = 2 )\n",
    "    plt.xlabel(\"Predicted Value\")\n",
    "    plt.ylabel(\"Actual Value\")\n",
    "    plt.title(nms[ii])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this works surprisingly well\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "mor = MultiOutputRegressor(RandomForestRegressor(1000, n_jobs = 4)).fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpreds = mor.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nms = [\"tau\", \"Fx\", \"Fy\"]\n",
    "\n",
    "for ii in range(3):\n",
    "    plt.scatter(y = Ytest[:,ii],x = mpreds[:,ii], s = 2 )\n",
    "    plt.xlabel(\"Predicted Value\")\n",
    "    plt.ylabel(\"Actual Value\")\n",
    "    plt.title(nms[ii])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "nms = [\"tau\", \"Fx\", \"Fy\"]\n",
    "\n",
    "for ii in range(3):\n",
    "    plt.scatter(y = Ytest[:,ii]- mpreds[:,ii], x = np.arange(len(Ytest[:,ii])), s = 2 )\n",
    "    plt.xlabel(\"Predicted Value\")\n",
    "    plt.ylabel(\"Actual Value\")\n",
    "    plt.title(nms[ii])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refref: next step -- neural network\n",
    "#from sklearn import cross_validation\n",
    "#from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#model = MLPRegressor(solver='lbfgs',alpha=0.001,hidden_layer_sizes=(150,)) \n",
    "#cross_validation.cross_val_score(model, X, Y, scoring='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# another possible way to rearrange the data\n",
    "ss = 12\n",
    "a = np.transpose(Vmat[\"ValSp3\"])\n",
    "bb = np.array(np.zeros(a.shape)).reshape([-1, ss])\n",
    "bb\n",
    "\n",
    "for kk in np.arange(0, a.shape[1], step = a.shape[0]):\n",
    "    bb[kk:(kk+a.shape[0]),0:ss] = a[0:, kk:(ss+kk)]  \n",
    "\n",
    "    \n",
    "forceAngle = pd.DataFrame(bb)\n",
    "forceAngle.columns = np.concatenate((new_cols, [\"F\", \"alpha\", \"tau\", \"cost\"]))\n",
    "forceAngle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
