{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callin Switzer\n",
    "## train NN with Jorge's new data\n",
    "## 23 Aug 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import subprocess\n",
    "import csv\n",
    "\n",
    "print(sys.version, \"\\n\")\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"last run on \" + str(now))\n",
    "\n",
    "#%qtconsole\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow successfully installed.\")\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow includes GPU support.\")\n",
    "\n",
    "# define directories\n",
    "baseDir = os.getcwd()\n",
    "dataDir = r'D:\\MothSimulations\\11c-AggressiveManeuver\\Qstore\\hws_am_con'\n",
    "figDir = r'D:\\Dropbox\\mothMachineLearning_dataAndFigs\\Figs'\n",
    "dataOutput = r'D:\\Dropbox\\mothMachineLearning_dataAndFigs\\DataOutput'\n",
    "if not os.path.exists(dataOutput):\n",
    "    os.mkdir(dataOutput)\n",
    "    \n",
    "    \n",
    "# open file explorer\n",
    "# aa = r'explorer /select,\"'\n",
    "# subprocess.Popen(str(aa + figDir + '\\\"'))\n",
    "# subprocess.Popen(str(aa + dataDir + '\\\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_fullpath(d):\n",
    "    return [os.path.join(d, f) for f in os.listdir(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir2 = r\"D:\\Dropbox\\mothMachineLearning_dataAndFigs\\DataOutput\"\n",
    "dataFiles = listdir_fullpath(dataDir2)\n",
    "dataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfList = []\n",
    "\n",
    "for dataFile in dataFiles:\n",
    "    bf2 = pd.read_hdf(dataFile, \"data\")\n",
    "    bf2[['simNum', 'timestep']] = bf2[['simNum', 'timestep']].astype(int)\n",
    "    bf2['count'] = bf2.groupby([\"simNum\", \"timestep\"]).cumcount()\n",
    "    sdf = bf2.loc[(bf2[\"count\"] == 0) | (bf2[\"count\"] == 99) | (bf2[\"count\"] == 50), :]\n",
    "    ss = sdf.pivot_table(index=['simNum', 'timestep'], columns = \"count\",aggfunc='first')\n",
    "    ss.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in ss.columns]\n",
    "    ss.reset_index(inplace=True) \n",
    "    dfList.append(ss)\n",
    "    print(dataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.concat(dfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.drop(columns=['F_99', 'alpha_99', \n",
    "                               'tau_99'])\n",
    "trainDF=trainDF.rename(columns = {'F_0':'F', \n",
    "                                 \"alpha_0\": \"alpha\", \n",
    "                                 \"tau_0\": \"tau\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.reset_index(inplace = True, drop = True)\n",
    "print(trainDF.shape)\n",
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.sort_values([\"timestep\", \"simNum\"], ascending = [True, True]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to fx and fy\n",
    "trainDF[\"Fx\"] = trainDF.F * np.cos(trainDF.alpha)\n",
    "trainDF[\"Fy\"] = trainDF.F * np.sin(trainDF.alpha)\n",
    "trainDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "X = trainDF.loc[:, [\"x_0\", \"y_0\", \"phi_0\", \"theta_0\", \"x_99\", \"y_99\", \"phi_99\", \"theta_99\", \"x_50\", \"y_50\", \"phi_50\", \"theta_50\"]]\n",
    "Y = trainDF.loc[:, [\"Fx\", \"Fy\", \"tau\"] ]\n",
    "\n",
    "# # # subtract x_0 and y_0 from everything\n",
    "# X.x_99 = X.x_99 - X.x_0\n",
    "# X.y_99 = X.y_99 - X.y_0\n",
    "\n",
    "# X.x_0 = X.y_0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scalerX = StandardScaler()  \n",
    "scalerY = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scalerX.fit(Xtrain)  \n",
    "scalerY.fit(Ytrain) \n",
    "\n",
    "Xtrain_scaled = scalerX.transform(Xtrain)  \n",
    "Ytrain_scaled = scalerY.transform(Ytrain)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "Xtest_scaled = scalerX.transform(Xtest)\n",
    "Ytest_scaled = scalerY.transform(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler  \n",
    "# scalerX = MinMaxScaler()  \n",
    "# scalerY = MinMaxScaler()  \n",
    "\n",
    "# # Don't cheat - fit only on training data\n",
    "# scalerX.fit(Xtrain)  \n",
    "# scalerY.fit(Ytrain) \n",
    "\n",
    "# Xtrain_scaled = scalerX.transform(Xtrain)  \n",
    "# Ytrain_scaled = scalerY.transform(Ytrain)  \n",
    "\n",
    "# # apply same transformation to test data\n",
    "# Xtest_scaled = scalerX.transform(Xtest)\n",
    "# Ytest_scaled = scalerY.transform(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Keras model with functional API, instead of Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# callcacks\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(Xtrain_scaled.shape[1],))\n",
    "\n",
    "x = Dense(400, activation='tanh')(inputs)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(16, activation='tanh')(x)\n",
    "predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=20, \n",
    "                          verbose=1, mode='auto', min_delta = 0.001)\n",
    "\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                    verbose = 2, batch_size=2**13, epochs = 1000, \n",
    "                    callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE')\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history.history['val_mean_squared_error'])+1),len(model_history.history['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining.png\"), dpi = 120, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(history, saveFig = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnpreds = model.predict(Xtest_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(1,3, figsize=(15, 5), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.3)\n",
    "fig.suptitle('Predicted vs. acutal ', fontsize=14, fontweight='bold')\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "nms = [\"Fx\", \"Fy\", \"tau\"]\n",
    "\n",
    "for ii in range(3):\n",
    "    # read in image\n",
    "    \n",
    "    try:\n",
    "        axs[ii].scatter(y = Ytest_scaled[:,ii],x = nnpreds[:,ii], s = 1 )\n",
    "        axs[ii].set_xlabel(\"Predicted Value (scaled)\")\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\\n(scaled)\")\n",
    "        axs[ii].set_title(nms[ii])\n",
    "        axs[ii].plot(Ytest_scaled[:,ii], Ytest_scaled[:,ii], 'b')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "fig.savefig(os.path.join(figDir, \"NNPreds_newData_control__50.png\"), dpi = 120, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(1,3, figsize=(15, 5), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.3)\n",
    "fig.suptitle('Predicted vs. acutal ', fontsize=14, fontweight='bold')\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "nms = [\"Fx\", \"Fy\", \"tau\"]\n",
    "\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "#cmaplist[0:2] = np.repeat((0.0,0.0,0.0,0.0), 2).reshape(-1,4)\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "#cmaplist[:,0:3] = np.transpose(np.vstack([np.linspace(1, 0, 256), np.linspace(1, 0, 256), np.linspace(1, 0, 256)]))\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(3):\n",
    "    # read in image\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest_scaled[:,ii],x = nnpreds[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].set_xlabel(\"Predicted Value (scaled)\")\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\\n(scaled)\")\n",
    "        axs[ii].set_title(nms[ii])\n",
    "        axs[ii].plot(Ytest_scaled[:,ii], Ytest_scaled[:,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "fig.savefig(os.path.join(figDir, \"NNPreds_newData_control_bigbatch_FxFy_hexbin_50.png\"), dpi = 120, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref: idea - predict whole paths, rather than just the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf2[['simNum', 'timestep']] = bf2[['simNum', 'timestep']].astype(int)\n",
    "\n",
    "bf2['count'] = bf2.groupby([\"simNum\", \"timestep\"]).cumcount()\n",
    "\n",
    "bf2.head()\n",
    "\n",
    "bf2.tail()\n",
    "\n",
    "sdf = bf2.loc[(bf2[\"count\"] == 0) | (bf2[\"count\"] == 99), :]\n",
    "sdf.head()\n",
    "\n",
    "ss = sdf.pivot_table(index='simNum', columns = \"count\",aggfunc='first')\n",
    "ss.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in ss.columns]\n",
    "ss[\"simNum\"] = ss.index\n",
    "ss.index.name = None\n",
    "ss.head()\n",
    "\n",
    "np.array(ss).shape\n",
    "\n",
    "np.vstack([np.array(ss), np.array(ss)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
