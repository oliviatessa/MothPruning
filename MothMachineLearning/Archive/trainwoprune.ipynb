{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('font',family='Times New Roman')\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import subprocess\n",
    "#import winsound\n",
    "import pickle\n",
    "import glob\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow successfully installed.\")\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow includes GPU support.\")\n",
    "print(sys.version, \"\\n\")\n",
    "now = datetime.now()\n",
    "print(\"last run on \" + str(now))\n",
    "\n",
    "# define directories\n",
    "baseDir = os.getcwd()\n",
    "#dataDir = r'D:\\MothSimulations\\11c-AggressiveManeuver\\Qstore\\hws_am_con'\n",
    "figDir = '/home/olivia/MothMachineLearning/dataAndFigs/Figs/'\n",
    "dataOutput = '/home/olivia/MothMachineLearning/dataAndFigs/DataOutput/'\n",
    "savedModels = '/home/olivia/MothMachineLearning/dataAndFigs/savedModels/'\n",
    "dataDir = '/home/olivia/MothMachineLearning/dataAndFigs/PythonGeneratedData_oneTorque/'\n",
    "if not os.path.exists(figDir):\n",
    "    os.mkdir(figDir)\n",
    "\n",
    "if not os.path.exists(dataOutput):\n",
    "    os.mkdir(dataOutput)\n",
    "if not os.path.exists(savedModels):\n",
    "    os.mkdir(savedModels)\n",
    "\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "# import\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# Keras callcacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras.backend as K\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i in np.arange(4):\n",
    "    print(i)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Make training and test set\n",
    "\n",
    "# get table names in database\n",
    "con1 = sqlite3.connect(os.path.join(dataDir, \"oneTorqueData.db\"))\n",
    "cursorObj = con1.cursor()\n",
    "res = cursorObj.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tableNames = [name[0] for name in res]\n",
    "con1.close()\n",
    "print(tableNames)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "con1 = sqlite3.connect(os.path.join(dataDir, \"oneTorqueData.db\"))\n",
    "trainDF = pd.read_sql_query(\"SELECT * FROM train\", con1)\n",
    "testDF = pd.read_sql_query(\"SELECT * FROM test\", con1)\n",
    "con1.close()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# double check for repeats!\n",
    "np.sum(trainDF.iloc[:, [16,17,18]].duplicated()) # 0 means no repeats\n",
    "\n",
    "print(trainDF.shape)\n",
    "trainDF.head()\n",
    "\n",
    "# rename columns to be consistent with other code\n",
    "trainDF.rename(columns={\"x0\" : \"x_0\", \"y0\" : \"y_0\", \"phi0\" : \"phi_0\", \"theta0\" : \"theta_0\", \n",
    "                        \"x_f\" : \"x_99\", \"y_f\" : \"y_99\", \"phi_f\" : \"phi_99\", \"theta_f\" : \"theta_99\", \n",
    "                        \"xd_0\" : \"x_dot_0\", \"yd_0\" : \"y_dot_0\", \"phid_0\" : \"phi_dot_0\", \"thetad_0\": \"theta_dot_0\", \n",
    "                        \"xd_f\" : \"x_dot_99\", \"yd_f\": \"y_dot_99\", \"phid_f\": \"phi_dot_99\", \"thetad_f\": \"theta_dot_99\", \n",
    "                        \"tau0\" : \"tau\"}, inplace=True)\n",
    "\n",
    "# rename columns to be consistent with other code\n",
    "testDF.rename(columns={\"x0\" : \"x_0\", \"y0\" : \"y_0\", \"phi0\" : \"phi_0\", \"theta0\" : \"theta_0\", \n",
    "                        \"x_f\" : \"x_99\", \"y_f\" : \"y_99\", \"phi_f\" : \"phi_99\", \"theta_f\" : \"theta_99\", \n",
    "                        \"xd_0\" : \"x_dot_0\", \"yd_0\" : \"y_dot_0\", \"phid_0\" : \"phi_dot_0\", \"thetad_0\": \"theta_dot_0\", \n",
    "                        \"xd_f\" : \"x_dot_99\", \"yd_f\": \"y_dot_99\", \"phid_f\": \"phi_dot_99\", \"thetad_f\": \"theta_dot_99\", \n",
    "                        \"tau0\" : \"tau\"}, inplace=True)\n",
    "\n",
    "# convert to fx and fy\n",
    "trainDF[\"Fx\"] = trainDF.F * np.cos(trainDF.alpha)\n",
    "trainDF[\"Fy\"] = trainDF.F * np.sin(trainDF.alpha)\n",
    "\n",
    "testDF[\"Fx\"] = testDF.F * np.cos(testDF.alpha)\n",
    "testDF[\"Fy\"] = testDF.F * np.sin(testDF.alpha)\n",
    "\n",
    "\n",
    "# make dataset\n",
    "X = trainDF.loc[:, [ \"phi_0\", \"theta_0\", \n",
    "                    \"x_99\", \"y_99\", \"phi_99\", \"theta_99\", \n",
    "                   \"x_dot_0\", \"y_dot_0\", \"phi_dot_0\", \"theta_dot_0\"]]\n",
    "\n",
    "Y = trainDF.loc[:, [\"Fx\", \"Fy\", \"tau\", \"x_dot_99\", \"y_dot_99\", \n",
    "                    \"phi_dot_99\", \"theta_dot_99\"] ]\n",
    "\n",
    "# make test dataset\n",
    "Xtest = testDF.loc[:, [ \"phi_0\", \"theta_0\", \n",
    "                    \"x_99\", \"y_99\", \"phi_99\", \"theta_99\", \n",
    "                   \"x_dot_0\", \"y_dot_0\", \"phi_dot_0\", \"theta_dot_0\"]]\n",
    "\n",
    "Ytest = testDF.loc[:, [\"Fx\", \"Fy\", \"tau\", \"x_dot_99\", \"y_dot_99\", \n",
    "                    \"phi_dot_99\", \"theta_dot_99\"] ]\n",
    "\n",
    "# val train split\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size=0.2, random_state = 123)\n",
    "\n",
    "# scale data \n",
    "scalerX = MinMaxScaler([-0.5, 0.5])  \n",
    "scalerY = MinMaxScaler([-0.5, 0.5])  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scalerX.fit(Xtrain)  \n",
    "scalerY.fit(Ytrain) \n",
    "\n",
    "Xtrain_scaled = scalerX.transform(Xtrain)  \n",
    "Ytrain_scaled = scalerY.transform(Ytrain)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "Xval_scaled = scalerX.transform(Xval)\n",
    "Yval_scaled = scalerY.transform(Yval)\n",
    "\n",
    "\n",
    "# final test data\n",
    "Xtest_scaled = scalerX.transform(Xtest)\n",
    "\n",
    "pd.DataFrame(Xtrain_scaled, columns = X.columns).head()\n",
    "\n",
    "___\n",
    "\n",
    "### Run the next two code blocks to prepare data for multi-GPU training\n",
    "\n",
    "def get_dataset(Xtrain_scaled, Ytrain_scaled, Xval_scaled, Yval_scaled, Xtest_scaled, Ytest):\n",
    "    batch_size = 4*(2**12)\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((Xtrain_scaled, Ytrain_scaled)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((Xval_scaled, Yval_scaled)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((Xtest_scaled, Ytest)).batch(batch_size),\n",
    "    )\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_dataset(Xtrain_scaled, Ytrain_scaled, Xval_scaled, Yval_scaled, Xtest_scaled, Ytest)\n",
    "\n",
    "___\n",
    "\n",
    "# Keras models\n",
    "\n",
    "# import\n",
    "#from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "#from keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Keras callcacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "#K.clear_session()\n",
    "\n",
    "___\n",
    "\n",
    "### Run the next code block to set up Multi-GPU strategy\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "___\n",
    "\n",
    "# create network\n",
    "def create_network(optimizer = 'rmsprop', \n",
    "                    numUnits = [400, 16], \n",
    "                    weightRegularization = 0.0, \n",
    "                    dropout_rate=0.1):\n",
    "    \n",
    "    '''\n",
    "    Create a feed forward network.  Assumes Xtrain & Ytrain have been created and scaled\n",
    "    \n",
    "    Params: \n",
    "    optimizer (str): choice of optimizer\n",
    "    numUnits (list): number of units in each hidden\n",
    "    weightRegularization (float): between 0 and 1\n",
    "    dropout_rate (float): between 0 and 1\n",
    "    \n",
    "    '''\n",
    "    #K.clear_session()\n",
    "    # Causes error when using multiple GPUs\n",
    "    \n",
    "    inputs = Input(shape=(Xtrain_scaled.shape[1],))    \n",
    "    \n",
    "    # add layers\n",
    "    for ii in np.arange(0, len(numUnits)):\n",
    "        if ii >= 1: \n",
    "            x = Dense(numUnits[ii], activation='tanh', \n",
    "                      kernel_regularizer=regularizers.l1(weightRegularization),use_bias=True)(x)\n",
    "\n",
    "        else: \n",
    "            x = Dense(numUnits[ii], activation='tanh',use_bias=True)(inputs)\n",
    "\n",
    "\n",
    "        # add dropout\n",
    "        if dropout_rate > 0: \n",
    "            x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    predictions = Dense(Ytrain_scaled.shape[1], activation='linear',use_bias=True)(x)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer = optimizer, metrics = ['mse'])\n",
    "\n",
    "    return(model)\n",
    "\n",
    "def format_e(n):\n",
    "    a = '%E' % n\n",
    "    return a.split('E')[0].rstrip('0').rstrip('.') + 'E' + a.split('E')[1]\n",
    "\n",
    "# Adjusting early stop from 0.000001 to 0.0001 to try to make my plot look more like Callin's \n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=50, \n",
    "                          verbose=1, mode='auto', min_delta = 0.000001)\n",
    "\n",
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),\n",
    "             model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "             model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE = '+ str(format_e(model_history.history['val_mean_squared_error'][-1])))\n",
    "    axs.set_ylabel('Mean squared error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(0,len(model_history.history['val_mean_squared_error']), 50), 50)\n",
    "    axs.legend(['train', 'validation'], loc='best')\n",
    "    plt.yscale('log') #logarithmic scale for y axis\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining_\" + modelName + \".pdf\"), dpi = 500, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_model_history_fromDict(model_history_dictionary, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(16,8))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history_dictionary['mean_squared_error'])+1),\n",
    "             model_history_dictionary['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history_dictionary['val_mean_squared_error'])+1),\n",
    "             model_history_dictionary['val_mean_squared_error'], alpha = 0.7)\n",
    "    axs.set_title('Model MSE = '+ str(format_e(model_history_dictionary['val_mean_squared_error'][-1])))\n",
    "    axs.set_ylabel('Mean squared error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(0,len(model_history_dictionary['val_mean_squared_error']), 50),50)\n",
    "    axs.legend(['train', 'validation'], loc=2)\n",
    "    plt.yscale('log') #logarithmic scale for y axis\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining_\" + modelName + \"_pruned.png\"), dpi = 120, bbox_inches='tight')\n",
    "        print(os.path.join(figDir, \"ModelTraining_\" + modelName + \"_pruned.png\"))\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "# train and trim weights\n",
    "\n",
    "modelParams = {\"optimizer\": 'rmsprop', \n",
    "              \"dropout_rate\" : 0, \n",
    "               \"numUnits\": [400, 400, 400, 16],\n",
    "               \"weightRegularization\": 0\n",
    "              }\n",
    "\n",
    "# Multi-GPU\n",
    "with strategy.scope():\n",
    "    model = create_network(**modelParams)\n",
    "\n",
    "# Single GPU\n",
    "#model = create_network(**modelParams)\n",
    "\n",
    "modeltimestamp = datetime.now().strftime(\"%Y_%m_%d__%I_%M_%S\")\n",
    "modelName = ''.join('{}_{}__'.format(key[0:3].capitalize(), val) for  key, val in modelParams.items()).\\\n",
    "                            replace(\"[\", \"\").replace(\"]\", \"\").replace(\", \", \"_\")[0:-2] + \"_\" + modeltimestamp\n",
    "print(modelName)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# # save scalers, to be used on test set\n",
    "scalerfileX = 'scalerX_fullact_' + modeltimestamp + '.pkl'\n",
    "pickle.dump(scalerX, open(os.path.join(dataOutput, scalerfileX), 'wb'))\n",
    "\n",
    "scalerfileY = 'scalerY_fullact_' + modeltimestamp + '.pkl'\n",
    "pickle.dump(scalerY, open(os.path.join(dataOutput, scalerfileY), 'wb'))\n",
    "\n",
    "# start training\n",
    "historyDict = {\"mean_squared_error\": [], \n",
    "               \"val_mean_squared_error\": []}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training with single GPU\n",
    "#history = model.fit(Xtrain_scaled, Ytrain_scaled, \n",
    "#                    validation_data=(Xval_scaled, Yval_scaled), \n",
    "#                    callbacks = [earlystop],\n",
    "#                    verbose = 2, batch_size=2**20, epochs = 1000)\n",
    "\n",
    "# Training with multiple GPUs\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks = [earlystop],\n",
    "                    verbose = 2, epochs = 1000)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "# save history\n",
    "historyDict[\"mean_squared_error\"].extend(history.history['mean_squared_error'])\n",
    "historyDict[\"val_mean_squared_error\"].extend(history.history['val_mean_squared_error'])\n",
    "\n",
    "plot_model_history_fromDict(historyDict)\n",
    "\n",
    "# save model\n",
    "model.save(os.path.join(savedModels,  modelName + '.h5'))\n",
    "\n",
    "# save weights\n",
    "wts = model.get_weights().copy()\n",
    "\n",
    "wtsFile = modelName + '_wts.pkl'\n",
    "pickle.dump(wts, open(os.path.join(dataOutput, wtsFile), 'wb'))\n",
    "\n",
    "# save history with same name as model\n",
    "historyFile = modeltimestamp + '_history.pkl'\n",
    "pickle.dump(historyDict, open(os.path.join(dataOutput, historyFile), 'wb'))\n",
    "\n",
    "#  plot error rates on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearn_V4",
   "language": "python",
   "name": "deeplearn_v4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
